# -*- coding: utf-8 -*-
"""sentiment-analysis-roberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bVo_bcMsqit5yptHkiDtuOjPq1JW2zN

# Step 0. Read in Data and NLTK Basics
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


plt.style.use('ggplot')

import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

# Read in data
df = pd.read_csv('sample_data/Reviews.csv')
print(df.shape)
df = df.head(1000)
print(df.shape)

df.head()

"""## Quick EDA"""

ax = df['Score'].value_counts().sort_index() \
    .plot(kind='bar',
          title='Count of Reviews by Stars',
          figsize=(10, 5))
ax.set_xlabel('Review Stars')
plt.show()

"""## Basic NLTK"""

example = df['Text'][50]
print(example)

tokens = nltk.word_tokenize(example)
tokens[:10]

tagged = nltk.pos_tag(tokens)
tagged[:10]

entities = nltk.chunk.ne_chunk(tagged)
entities.pprint()

"""# Step 1. VADER Seniment Scoring

We will use NLTK's `SentimentIntensityAnalyzer` to get the neg/neu/pos scores of the text.

- This uses a "bag of words" approach:
    1. Stop words are removed
    2. each word is scored and combined to a total score.
"""

from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
nltk.download('vader_lexicon')
sia = SentimentIntensityAnalyzer()

sia.polarity_scores('I am so happy!')

sia.polarity_scores('This is the worst thing ever.')

sia.polarity_scores(example)

# Run the polarity score on the entire dataset
res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    text = row['Text']
    myid = row['Id']
    res[myid] = sia.polarity_scores(text)

vaders = pd.DataFrame(res).T
vaders = vaders.reset_index().rename(columns={'index': 'Id'})
vaders = vaders.merge(df, how='left')

# Now we have sentiment score and metadata
vaders.head()

"""## Plot VADER results"""

ax = sns.barplot(data=vaders, x='Score', y='compound')
ax.set_title('Compund Score by Amazon Star Review')
plt.show()

fig, axs = plt.subplots(1, 3, figsize=(12, 3))
sns.barplot(data=vaders, x='Score', y='pos', ax=axs[0])
sns.barplot(data=vaders, x='Score', y='neu', ax=axs[1])
sns.barplot(data=vaders, x='Score', y='neg', ax=axs[2])
axs[0].set_title('Positive')
axs[1].set_title('Neutral')
axs[2].set_title('Negative')
plt.tight_layout()
plt.show()

"""# Step 3. Roberta Pretrained Model

- Use a model trained of a large corpus of data.
- Transformer model accounts for the words but also the context related to other words.
"""

from transformers import AutoTokenizer
from transformers import AutoModelForSequenceClassification
from scipy.special import softmax
import torch

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

# VADER results on example
print(example)
sia.polarity_scores(example)

# Run for Roberta Model
encoded_text = tokenizer(example, return_tensors='pt')
output = model(**encoded_text)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
scores_dict = {
    'roberta_neg' : scores[0],
    'roberta_neu' : scores[1],
    'roberta_pos' : scores[2]
}
print(scores_dict)

def polarity_scores_roberta(example):
    encoded_text = tokenizer(example, return_tensors='pt')
    with torch.no_grad():
        output = model(**encoded_text)
    scores = softmax(output.logits[0].numpy())
    compound = scores[2] - scores[0]  # positive score - negative score
    scores_dict = {
        'roberta_neg': scores[0],
        'roberta_neu': scores[1],
        'roberta_pos': scores[2],
        'roberta_compound': compound
    }
    return scores_dict

res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    try:
        text = row['Text']
        myid = row['Id']
        vader_result = sia.polarity_scores(text)
        vader_result_rename = {f"vader_{key}": value for key, value in vader_result.items()}
        roberta_result = polarity_scores_roberta(text)
        both = {**vader_result_rename, **roberta_result}
        res[myid] = both
    except RuntimeError as e:
        print(f'Broke for id {myid} with error {e}')

results_df = pd.DataFrame.from_dict(res, orient='index').reset_index().rename(columns={'index': 'Id'})
results_df = results_df.merge(df, how='left')

"""## Compare Scores between models"""

results_df.columns

import seaborn as sns

# Setting the aesthetic style of the plots
sns.set_style("whitegrid")

# Plotting the distribution of compound scores
plt.figure(figsize=(14, 6))

# VADER compound score distribution
sns.kdeplot(results_df['vader_compound'], label='VADER Compound', fill=True)

# RoBERTa compound score distribution
sns.kdeplot(results_df['roberta_compound'], label='RoBERTa Compound', fill=True)

plt.title('Distribution of Compound Scores: VADER vs RoBERTa')
plt.xlabel('Compound Score')
plt.ylabel('Density')
plt.legend()
plt.show()

# Focus on sentiment scores for correlation
sentiment_scores = results_df[['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound',
                               'roberta_neg', 'roberta_neu', 'roberta_pos', 'roberta_compound']]

# Calculating the correlation matrix
correlation_matrix = sentiment_scores.corr()

# Plotting the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Sentiment Score Correlation: VADER vs RoBERTa')
plt.show()

"""Logistic Regression"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

"""# Step 2: Preparing the **Data**"""

# Assuming 'Score' ranges from 1 to 5; simplifying to binary classification for example
df['Sentiment'] = df['Score'].apply(lambda x: 1 if x > 3 else 0)

"""# Step 3: Tokenize Text and Prepare for **Doc2Vec**"""

# Tokenizing text data and tagging each text with unique IDs
tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df['Text'])]

"""# **Step 4: Train Doc2Vec Model**"""

# Defining and training a Doc2Vec model
model_d2v = Doc2Vec(vector_size=100, alpha=0.025, min_alpha=0.00025, min_count=1, dm=1)

model_d2v.build_vocab(tagged_data)

# Training the Doc2Vec model
for epoch in range(10):
    model_d2v.train(tagged_data, total_examples=model_d2v.corpus_count, epochs=model_d2v.epochs)
    # Decrease the learning rate
    model_d2v.alpha -= 0.0002
    # Fix the learning rate, no decay
    model_d2v.min_alpha = model_d2v.alpha

model_d2v.save("d2v.model")
print("Model Saved")

"""# Step 5: Prepare Feature Vectors for Logistic **Regression**"""

# Generating feature vectors for each document
vectors = [model_d2v.infer_vector(doc.words) for doc in tagged_data]

X = np.array(vectors)
y = df['Sentiment'].values

"""# **Step 6: Split the Data**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# **Step 7: Train Logistic Regression Model**"""

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train, y_train)

"""# **Step 8: Model Evaluation**"""

predictions = lr.predict(X_test)

print("Accuracy:", accuracy_score(y_test, predictions))
print(classification_report(y_test, predictions))

"""**Random Forest**"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import word_tokenize  # Make sure to import word_tokenize

nltk.download('punkt')  # This downloads the necessary tokenizers

# Assuming 'Score' ranges from 1 to 5; simplifying to binary classification
df['Sentiment'] = df['Score'].apply(lambda x: 1 if x > 3 else 0)

# Prepare the text data
texts = df['Text'].apply(lambda x: x.lower())

# Vectorizing text data using TF-IDF
tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english', min_df=2)
X = tfidf.fit_transform(texts).toarray()
y = df['Sentiment'].values

# Splitting data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Defining and training the Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Making predictions
predictions = rf.predict(X_test)

# Evaluating the model
print("Accuracy:", accuracy_score(y_test, predictions))
print(classification_report(y_test, predictions))

import openai

# Replace 'your-api-key' with your actual OpenAI API key
openai.api_key = 'your-api-key'

def analyze_sentiment_gpt3(text):
    response = openai.Completion.create(
      engine="text-davinci-002",  # As of my last training cut-off, check for the latest model version
      prompt=f"Analyze the sentiment of this text: \"{text}\". Is it positive, negative, or neutral? Give me values from 0 to 1 to the particular field",
      max_tokens=60
    )
    # The response will contain the text with the sentiment analysis
    return response.choices[0].text.strip()

# Example function to apply sentiment scoring to a DataFrame
def append_gpt_sentiment(df):
    df[['gpt_neg', 'gpt_neu', 'gpt_pos']] = df['Text'].apply(
        lambda text: pd.Series(gpt_sentiment_scores(text))
    )
    return df

# Applying the function to your DataFrame
results_df = append_gpt_sentiment(results_df)

results_df

"""# Step 3. Combine and compare"""

sns.pairplot(data=results_df,
             vars=['vader_neg', 'vader_neu', 'vader_pos', 'vader_compound',
                  'roberta_neg', 'roberta_neu', 'roberta_pos', 'roberta_compound'],
            hue='Score',
            palette='tab10')
plt.show()



"""# Step 4: Review Examples:

- Positive 1-Star and Negative 5-Star Reviews

Lets look at some examples where the model scoring and review score differ the most.
"""

results_df.query('Score == 1') \
    .sort_values('roberta_pos', ascending=False)['Text'].values[0]

results_df.query('Score == 1') \
    .sort_values('vader_pos', ascending=False)['Text'].values[0]

# nevative sentiment 5-Star view

results_df.query('Score == 5') \
    .sort_values('roberta_neg', ascending=False)['Text'].values[0]

results_df.query('Score == 5') \
    .sort_values('vader_neg', ascending=False)['Text'].values[0]

"""# Extra: The Transformers Pipeline
- Quick & easy way to run sentiment predictions
"""

from transformers import pipeline

sent_pipeline = pipeline("sentiment-analysis")

sent_pipeline('I love sentiment analysis!')

sent_pipeline('Make sure to like and subscribe!')

sent_pipeline('booo')

"""# The End"""